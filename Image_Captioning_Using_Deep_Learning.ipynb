{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Data Preparation**"
      ],
      "metadata": {
        "id": "OgnewqfyDIvI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbasI4uMNyas",
        "outputId": "94d3d51e-84bc-40e2-c1c7-4453b6eb900e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-08-12 16:47:50--  http://images.cocodataset.org/zips/train2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.29.158, 16.15.176.85, 3.5.25.143, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.29.158|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13510573713 (13G) [application/zip]\n",
            "Saving to: ‘coco/images/train2014.zip’\n",
            "\n",
            "train2014.zip       100%[===================>]  12.58G  75.5MB/s    in 3m 2s   \n",
            "\n",
            "2025-08-12 16:50:52 (70.8 MB/s) - ‘coco/images/train2014.zip’ saved [13510573713/13510573713]\n",
            "\n",
            "--2025-08-12 16:50:52--  http://images.cocodataset.org/zips/val2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.134.33, 3.5.27.184, 3.5.30.204, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.134.33|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: ‘coco/images/val2014.zip’\n",
            "\n",
            "val2014.zip         100%[===================>]   6.19G  58.8MB/s    in 1m 44s  \n",
            "\n",
            "2025-08-12 16:52:36 (61.1 MB/s) - ‘coco/images/val2014.zip’ saved [6645013297/6645013297]\n",
            "\n",
            "--2025-08-12 16:52:36--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.202.33, 52.216.50.57, 52.217.65.20, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.202.33|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252872794 (241M) [application/zip]\n",
            "Saving to: ‘coco/annotations/annotations_trainval2014.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.16M  52.0MB/s    in 4.6s    \n",
            "\n",
            "2025-08-12 16:52:41 (52.0 MB/s) - ‘coco/annotations/annotations_trainval2014.zip’ saved [252872794/252872794]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create folders to keep things organized\n",
        "!mkdir -p coco/images\n",
        "!mkdir -p coco/annotations\n",
        "\n",
        "# Download training images\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip -P coco/images/\n",
        "\n",
        "# Download validation images\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip -P coco/images/\n",
        "\n",
        "# Download captions (annotations)\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip -P coco/annotations/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxNyDQC8N3HG",
        "outputId": "d0764b5f-32cc-4229-c8a4-6510b7272687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace coco/images/train2014/COCO_train2014_000000270070.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n",
            "A\n",
            "A\n",
            "replace coco/images/val2014/COCO_val2014_000000324670.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace coco/annotations/annotations/instances_train2014.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Unzip training images\n",
        "!unzip -q coco/images/train2014.zip -d coco/images/\n",
        "\n",
        "# Unzip validation images\n",
        "!unzip -q coco/images/val2014.zip -d coco/images/\n",
        "\n",
        "# Unzip annotation file\n",
        "!unzip -q coco/annotations/annotations_trainval2014.zip -d coco/annotations/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwTHCNL24USl",
        "outputId": "702f8dea-1a4e-4b00-93e5-dbe2936e66de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: 82783\n",
            "Validation images: 40504\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "train_dir = 'coco/images/train2014'\n",
        "val_dir = 'coco/images/val2014'\n",
        "\n",
        "print(\"Training images:\", len(os.listdir(train_dir)))\n",
        "print(\"Validation images:\", len(os.listdir(val_dir)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U86S_Cuucw_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3201664f-65a9-4400-b12b-6af03f016053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: COCO_train2014_000000318556.jpg\n",
            "  Caption 1: A very clean and well decorated empty bathroom\n",
            "  Caption 2: A blue and white bathroom with butterfly themed wall tiles.\n",
            "  Caption 3: A bathroom with a border of butterflies and blue paint on the walls above it.\n",
            "  Caption 4: An angled view of a beautifully decorated bathroom.\n",
            "  Caption 5: A clock that blends in with the wall hangs in a bathroom. \n",
            "\n",
            "Image: COCO_train2014_000000116100.jpg\n",
            "  Caption 1: A panoramic view of a kitchen and all of its appliances.\n",
            "  Caption 2: A panoramic photo of a kitchen and dining room\n",
            "  Caption 3: A wide angle view of the kitchen work area\n",
            "  Caption 4: multiple photos of a brown and white kitchen. \n",
            "  Caption 5: A kitchen that has a checkered patterned floor and white cabinets.\n",
            "\n",
            "Image: COCO_train2014_000000379340.jpg\n",
            "  Caption 1: A graffiti-ed stop sign across the street from a red car \n",
            "  Caption 2: A vandalized stop sign and a red beetle on the road\n",
            "  Caption 3: A red stop sign with a Bush bumper sticker under the word stop.\n",
            "  Caption 4: A stop sign that has been vandalized is pictured in front of a parked car.\n",
            "  Caption 5: A street sign modified to read stop bush.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Load captions\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Path to the JSON file\n",
        "caption_path = 'coco/annotations/annotations/captions_train2014.json'\n",
        "\n",
        "# Open and load the file\n",
        "with open(caption_path, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Create a dictionary: {image_id: [list_of_captions]}\n",
        "captions_dict = {}\n",
        "\n",
        "for item in annotations['annotations']:\n",
        "    img_id = item['image_id']\n",
        "    caption = item['caption']\n",
        "    full_img_id = 'COCO_train2014_' + str(img_id).zfill(12) + '.jpg'\n",
        "\n",
        "    if full_img_id not in captions_dict:\n",
        "        captions_dict[full_img_id] = []\n",
        "\n",
        "    captions_dict[full_img_id].append(caption)\n",
        "\n",
        "# Show sample image ID and captions\n",
        "for k, v in list(captions_dict.items())[:3]:\n",
        "    print(\"Image:\", k)\n",
        "    for i, cap in enumerate(v):\n",
        "        print(f\"  Caption {i+1}:\", cap)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def clean_caption(caption):\n",
        "    caption = caption.lower()  # make lowercase\n",
        "    caption = caption.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
        "    caption = caption.split()  # split into words\n",
        "    caption = [word for word in caption if word.isalpha()]  # remove non-alphabetic\n",
        "    caption = ' '.join(caption)  # join back into sentence\n",
        "    caption = 'startseq ' + caption + ' endseq'  # add start and end tokens\n",
        "    return caption\n",
        "\n",
        "for img_id, captions in captions_dict.items():\n",
        "    cleaned_captions = []\n",
        "    for caption in captions:\n",
        "        cleaned = clean_caption(caption)\n",
        "        cleaned_captions.append(cleaned)\n",
        "    captions_dict[img_id] = cleaned_captions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "98s1EXBrPtPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Flatten all captions into a single list\n",
        "all_captions = []\n",
        "for caption_list in captions_dict.values():\n",
        "    all_captions.extend(caption_list)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<unk>\")  # <unk> is for unknown words\n",
        "tokenizer.fit_on_texts(all_captions)  # learn the word-index mapping\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OjJjtYTaPwVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Convert all captions to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(all_captions)\n",
        "\n",
        "# Find the maximum length of any caption\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "print(max_length)\n",
        "\n",
        "# Pad the sequences with 0s at the end\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QADPAbn0PzV2",
        "outputId": "5b2e9eb9-d5d9-4460-9644-48ff7bcc31ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_caption = \"startseq a dog playing in park endseq\"\n",
        "sequence = tokenizer.texts_to_sequences([sample_caption])\n",
        "print(\"Sequence:\", sequence)\n",
        "padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "print(\"Padded Sequence:\", padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHrn3KHVP1Tf",
        "outputId": "3d8361c2-f9d0-4aa3-b8e4-92c45bd417d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence: [[3, 2, 47, 57, 8, 139, 4]]\n",
            "Padded Sequence: [[  3   2  47  57   8 139   4   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # for progress bar\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(299, 299))       # Resize\n",
        "    img_array = image.img_to_array(img)                           # Convert to array\n",
        "    img_array = np.expand_dims(img_array, axis=0)                 # Add batch dimension\n",
        "    img_array = preprocess_input(img_array)                       # Normalize\n",
        "    return img_array\n",
        "\n",
        "# Folder containing all images\n",
        "images_folder = \"/content/coco/images/train2014\"\n",
        "\n",
        "# List of all image filenames (you can limit for testing)\n",
        "image_files = os.listdir(images_folder)\n",
        "\n",
        "# Dictionary to store preprocessed images\n",
        "preprocessed_images = {}\n",
        "\n",
        "# Loop through all image files\n",
        "for img_name in tqdm(image_files[:1000]):  # Limit to 1000 for now (optional)\n",
        "    img_path = os.path.join(images_folder, img_name)\n",
        "    try:\n",
        "        preprocessed_images[img_name] = preprocess_image(img_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_name}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgrhXkhfP2l5",
        "outputId": "b2985fac-b467-4726-aed7-5cef00f8d75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:10<00:00, 97.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/coco/images/train2014/COCO_train2014_000000000009.jpg'\n",
        "preprocessed_img = preprocess_image(img_path)\n",
        "print(\"Shape:\", preprocessed_img.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zckLVI82P4wj",
        "outputId": "c3b44be3-9530-4393-9096-fb176f4d11af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (1, 299, 299, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step-2: Feature italicized text Extraction**"
      ],
      "metadata": {
        "id": "l4RIYDFWR4gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# 1️⃣ Load the InceptionV3 model pre-trained on ImageNet\n",
        "base_model = InceptionV3(weights='imagenet')\n",
        "\n",
        "# 2️⃣ Remove the last classification layer\n",
        "model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vee2l_abR4Fl",
        "outputId": "f7d2a3e0-a206-4af9-bc63-07f89049bab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m96112376/96112376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(img_path):\n",
        "    # Load image with target size\n",
        "    img = Image.open(img_path).resize((299, 299))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    # Preprocess for InceptionV3\n",
        "    img_array = preprocess_input(img_array)\n",
        "    return img_array\n"
      ],
      "metadata": {
        "id": "kQsW8FGGRxtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(img_dir, limit=None):\n",
        "    features = {}\n",
        "    img_names = os.listdir(img_dir)\n",
        "\n",
        "    if limit:\n",
        "        img_names = img_names[:limit]  # only take the first `limit` images\n",
        "\n",
        "    for img_name in tqdm(img_names):\n",
        "        img_path = os.path.join(img_dir, img_name)\n",
        "        try:\n",
        "            img_array = preprocess_image(img_path)\n",
        "            feature_vector = model.predict(img_array, verbose=0)\n",
        "            features[img_name] = feature_vector.flatten()\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_name}: {e}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "# Example usage:\n",
        "image_dir = \"/content/coco/images/train2014\"\n",
        "image_features = extract_features(image_dir, limit=500)  # process only first 500 images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqoHw7NPW1IJ",
        "outputId": "80746e2f-5429-4afa-b0a2-713bedc0198e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▋      | 182/500 [01:15<01:30,  3.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing COCO_train2014_000000578250.jpg: Graph execution error:\n",
            "\n",
            "Detected at node convolution defined at (most recent call last):\n",
            "<stack traces unavailable>\n",
            "Depth of input must be a multiple of depth of filter: 1 vs 3\n",
            "\n",
            "Stack trace for op definition: \n",
            "File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "File \"<frozen runpy>\", line 88, in _run_code\n",
            "File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "File \"/tmp/ipython-input-611061184.py\", line 16, in <cell line: 0>\n",
            "File \"/tmp/ipython-input-611061184.py\", line 7, in extract_features\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 183, in call\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 648, in call\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py\", line 250, in call\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py\", line 240, in convolution_op\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py\", line 1342, in conv\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py\", line 338, in conv\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py\", line 326, in _conv_xla\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py\", line 313, in _conv\n",
            "\n",
            "\t [[{{node convolution}}]]\n",
            "\ttf2xla conversion failed while converting __inference__conv_xla_15272[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n",
            "\t [[functional_1/conv2d_1/StatefulPartitionedCall]] [Op:__inference_one_step_on_data_distributed_17481]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 201/500 [01:23<01:28,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing COCO_train2014_000000066642.jpg: Graph execution error:\n",
            "\n",
            "Detected at node convolution defined at (most recent call last):\n",
            "<stack traces unavailable>\n",
            "Depth of input must be a multiple of depth of filter: 1 vs 3\n",
            "\n",
            "Stack trace for op definition: \n",
            "File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "File \"<frozen runpy>\", line 88, in _run_code\n",
            "File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "File \"/tmp/ipython-input-611061184.py\", line 16, in <cell line: 0>\n",
            "File \"/tmp/ipython-input-611061184.py\", line 7, in extract_features\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 183, in call\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\", line 648, in call\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py\", line 250, in call\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py\", line 240, in convolution_op\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py\", line 1342, in conv\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py\", line 338, in conv\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py\", line 326, in _conv_xla\n",
            "File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py\", line 313, in _conv\n",
            "\n",
            "\t [[{{node convolution}}]]\n",
            "\ttf2xla conversion failed while converting __inference__conv_xla_15272[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\n",
            "\t [[functional_1/conv2d_1/StatefulPartitionedCall]] [Op:__inference_one_step_on_data_distributed_17481]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [03:27<00:00,  2.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"image_features.pkl\", \"wb\") as f:\n",
        "    pickle.dump(image_features, f)\n",
        "print(f\"Saved features for {len(image_features)} images.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjH3v8jzXwkI",
        "outputId": "fa8cacba-940a-43f3-9bfe-58630ed795cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features for 498 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step-3: Preparing Captions for Training**"
      ],
      "metadata": {
        "id": "zfDNH6EQYfvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# ===== 1️⃣ Load captions =====\n",
        "captions_path = \"/content/coco/annotations/annotations/captions_train2014.json\"\n",
        "with open(captions_path, 'r') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# ===== 2️⃣ Clean caption =====\n",
        "def clean_caption(caption):\n",
        "    caption = caption.lower()\n",
        "    caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
        "    caption = ' '.join([word for word in caption.split() if word.isalpha()])\n",
        "    return caption\n",
        "\n",
        "# ===== 3️⃣ Build captions_dict with cleaning + special tokens =====\n",
        "captions_dict = {}\n",
        "for ann in captions_data['annotations']:\n",
        "    img_id = ann['image_id']\n",
        "    cleaned = clean_caption(ann['caption'])\n",
        "    caption = f\"<start> {cleaned} <end>\"\n",
        "    img_filename = f\"COCO_train2014_{img_id:012d}.jpg\"\n",
        "    captions_dict.setdefault(img_filename, []).append(caption)\n",
        "\n",
        "print(f\"Loaded captions for {len(captions_dict)} images.\")\n",
        "\n",
        "# ===== 4️⃣ Tokenize =====\n",
        "all_captions = [cap for caps in captions_dict.values() for cap in caps]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "# Convert captions to integer sequences\n",
        "for img in captions_dict:\n",
        "    captions_dict[img] = tokenizer.texts_to_sequences(captions_dict[img])\n",
        "\n",
        "# ===== 5️⃣ Max caption length =====\n",
        "max_length = max(len(seq) for caps in captions_dict.values() for seq in caps)\n",
        "print(\"Max caption length:\", max_length)\n",
        "\n",
        "# ===== 6️⃣ Prepare training data =====\n",
        "def create_sequences(tokenizer, max_length, captions_dict, image_features):\n",
        "    X1, X2, y = [], [], []\n",
        "    for img, caps in captions_dict.items():\n",
        "        if img not in image_features:  # skip images without features\n",
        "            continue\n",
        "        feature = image_features[img]\n",
        "        for seq in caps:\n",
        "            for i in range(1, len(seq)):\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                X1.append(feature)\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "# Example usage (make sure image_features is ready from Step 2)\n",
        "X1, X2, y = create_sequences(tokenizer, max_length, captions_dict, image_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9AUGQ-daSx_",
        "outputId": "fc48356d-bd19-4862-86f9-7ad70f671f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded captions for 82783 images.\n",
            "Vocabulary size: 24383\n",
            "Max caption length: 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vksQagtaWPy",
        "outputId": "a59bcf3e-7bdd-4407-96a2-08d0fe36182d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Building the Model**"
      ],
      "metadata": {
        "id": "SJL_Pu9hFJOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Dropout, Add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Attention\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "# ===== Encoder =====\n",
        "def build_model(vocab_size, max_length, embedding_dim=256, units=256):\n",
        "    # Image feature input (from InceptionV3 output in Step 2)\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    fe1 = Dense(units, activation='relu')(inputs1)  # transform feature vector\n",
        "    fe2 = Dropout(0.5)(fe1)\n",
        "\n",
        "    # Sequence input (caption tokens)\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(units, return_sequences=True)(se2)\n",
        "\n",
        "    # ===== Attention mechanism =====\n",
        "    # Expand image features to sequence length for attention\n",
        "    fe2_expanded = Lambda(lambda x: tf.expand_dims(x, 1))(fe2)\n",
        "\n",
        "    attention_out = Attention()([se3, fe2_expanded])  # context vector from image features\n",
        "\n",
        "    # Combine attention output with sequence features\n",
        "    decoder_combined = Add()([se3, attention_out])\n",
        "    decoder_lstm = LSTM(units)(decoder_combined)\n",
        "\n",
        "    # ===== Output layer =====\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder_lstm)\n",
        "\n",
        "    # ===== Build & compile model =====\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "model = build_model(vocab_size, max_length)\n",
        "# model.summary()\n"
      ],
      "metadata": {
        "id": "2NLNPKqAb3_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Training the Model**"
      ],
      "metadata": {
        "id": "1vQyGSm4E95z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you already have:\n",
        "# X1 → image features (shape: num_samples × 2048)\n",
        "# X2 → caption input sequences (shape: num_samples × max_length)\n",
        "# y  → next word indices (shape: num_samples × 1)\n",
        "\n",
        "# 1️⃣ Split into train and validation\n",
        "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n",
        "    X1, X2, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 2️⃣ Build model (from Step 4)\n",
        "model = build_model(vocab_size, max_length, embedding_dim=256, units=256)\n",
        "\n",
        "# 3️⃣ Compile with Adam optimizer\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 4️⃣ Train model\n",
        "history = model.fit(\n",
        "    [X1_train, X2_train], y_train,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    validation_data=([X1_val, X2_val], y_val),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Optional: Save trained model\n",
        "model.save(\"image_caption_model.h5\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgNgRXzgcQfl",
        "outputId": "86a7d9b3-cc26-476b-a9ba-d07750647467"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 51, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 1s/step - accuracy: 0.1400 - loss: 6.5909 - val_accuracy: 0.1921 - val_loss: 5.4685\n",
            "Epoch 2/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 1s/step - accuracy: 0.2127 - loss: 5.0368 - val_accuracy: 0.2676 - val_loss: 4.7875\n",
            "Epoch 3/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 1s/step - accuracy: 0.2744 - loss: 4.3977 - val_accuracy: 0.2891 - val_loss: 4.5450\n",
            "Epoch 4/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 1s/step - accuracy: 0.2968 - loss: 4.0691 - val_accuracy: 0.3090 - val_loss: 4.4546\n",
            "Epoch 5/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 1s/step - accuracy: 0.3187 - loss: 3.8285 - val_accuracy: 0.3228 - val_loss: 4.3882\n",
            "Epoch 6/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 1s/step - accuracy: 0.3285 - loss: 3.6381 - val_accuracy: 0.3317 - val_loss: 4.3102\n",
            "Epoch 7/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 1s/step - accuracy: 0.3441 - loss: 3.4364 - val_accuracy: 0.3314 - val_loss: 4.2512\n",
            "Epoch 8/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 1s/step - accuracy: 0.3524 - loss: 3.2707 - val_accuracy: 0.3357 - val_loss: 4.2373\n",
            "Epoch 9/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 1s/step - accuracy: 0.3609 - loss: 3.1245 - val_accuracy: 0.3380 - val_loss: 4.2489\n",
            "Epoch 10/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 1s/step - accuracy: 0.3707 - loss: 2.9972 - val_accuracy: 0.3425 - val_loss: 4.2382\n",
            "Epoch 11/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 1s/step - accuracy: 0.3896 - loss: 2.8399 - val_accuracy: 0.3459 - val_loss: 4.2472\n",
            "Epoch 12/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 1s/step - accuracy: 0.3986 - loss: 2.7391 - val_accuracy: 0.3506 - val_loss: 4.2686\n",
            "Epoch 13/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 1s/step - accuracy: 0.4135 - loss: 2.6255 - val_accuracy: 0.3485 - val_loss: 4.2850\n",
            "Epoch 14/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 1s/step - accuracy: 0.4273 - loss: 2.5073 - val_accuracy: 0.3478 - val_loss: 4.3172\n",
            "Epoch 15/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 1s/step - accuracy: 0.4534 - loss: 2.3830 - val_accuracy: 0.3520 - val_loss: 4.3268\n",
            "Epoch 16/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 1s/step - accuracy: 0.4643 - loss: 2.2873 - val_accuracy: 0.3411 - val_loss: 4.3619\n",
            "Epoch 17/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 1s/step - accuracy: 0.4804 - loss: 2.2053 - val_accuracy: 0.3483 - val_loss: 4.3916\n",
            "Epoch 18/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 1s/step - accuracy: 0.4903 - loss: 2.1355 - val_accuracy: 0.3542 - val_loss: 4.4490\n",
            "Epoch 19/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 1s/step - accuracy: 0.5127 - loss: 2.0281 - val_accuracy: 0.3446 - val_loss: 4.4694\n",
            "Epoch 20/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 1s/step - accuracy: 0.5262 - loss: 1.9661 - val_accuracy: 0.3480 - val_loss: 4.5168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Model Evaluation**"
      ],
      "metadata": {
        "id": "zi8J_lAhE2Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Example reference captions for 2 images\n",
        "y_true = [\n",
        "    [[\"a\", \"cat\", \"on\", \"a\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]],\n",
        "    [[\"a\", \"man\", \"riding\", \"a\", \"bike\"], [\"a\", \"person\", \"on\", \"a\", \"bicycle\"]]\n",
        "]\n",
        "\n",
        "# Example generated captions for 2 images\n",
        "y_pred = [\n",
        "    [\"a\", \"cat\", \"on\", \"a\", \"mat\"],\n",
        "    [\"a\", \"man\", \"riding\", \"a\", \"bicycle\"]\n",
        "]\n",
        "\n",
        "# Calculate BLEU scores\n",
        "bleu1 = corpus_bleu(y_true, y_pred, weights=(1.0, 0, 0, 0))\n",
        "bleu2 = corpus_bleu(y_true, y_pred, weights=(0.5, 0.5, 0, 0))\n",
        "bleu3 = corpus_bleu(y_true, y_pred, weights=(0.33, 0.33, 0.33, 0))\n",
        "bleu4 = corpus_bleu(y_true, y_pred, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "print(f\"BLEU-1: {bleu1:.4f}\")\n",
        "print(f\"BLEU-2: {bleu2:.4f}\")\n",
        "print(f\"BLEU-3: {bleu3:.4f}\")\n",
        "print(f\"BLEU-4: {bleu4:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJsGOq1__ZwR",
        "outputId": "5c2264db-a92f-4b3e-afa9-af1ee6e165e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1: 1.0000\n",
            "BLEU-2: 1.0000\n",
            "BLEU-3: 0.9416\n",
            "BLEU-4: 0.8891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Fine-tuning"
      ],
      "metadata": {
        "id": "zm6ozv191Pnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Fine-tuning (instructions + code snippets)\n",
        "# If you decide to fine-tune CNN: re-create InceptionV3 with include_top=False and unfreeze top layers.\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Example snippet (do NOT run unless you want to re-extract or fine-tune)\n",
        "base = InceptionV3(weights='imagenet', include_top=False)  # conv base only\n",
        "# unfreeze last N layers:\n",
        "for layer in base.layers[:-50]:\n",
        "    layer.trainable = False\n",
        "for layer in base.layers[-50:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# attach a small head if you want to fine-tune end-to-end; typically you re-run feature extraction after small-finetune\n",
        "print('Prepared InceptionV3 for fine-tuning (last 50 layers trainable).')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngbaK99y8yrs",
        "outputId": "f108cd5c-f9f2-4e08-a931-afd73fd7dbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Prepared InceptionV3 for fine-tuning (last 50 layers trainable).\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}